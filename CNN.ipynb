{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convolutional Neural Network (CNN) for Image Classification",
   "id": "89062689b58ed041"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:10:43.447497Z",
     "start_time": "2025-07-29T09:10:43.441374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fontTools.misc.plistlib import end_date\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ],
   "id": "9d7e9d248522f25b",
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-29T09:10:44.002343Z",
     "start_time": "2025-07-29T09:10:43.987581Z"
    }
   },
   "source": "tf.__version__",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.2'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1: Data Preprocessing",
   "id": "a95ac61f09bdc5f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing the Training set",
   "id": "6e3f4bb24975c034"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:10:45.224237Z",
     "start_time": "2025-07-29T09:10:44.800762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply transformations to the training set images to avoid overfitting\n",
    "# Also named as data augmentation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,  # Normalize pixel values to [0, 1]; Feature scaling\n",
    "    shear_range = 0.2,  # Shear transformation; \n",
    "    zoom_range = 0.2,  # Zoom transformation\n",
    "    horizontal_flip = True  # Random horizontal flip\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'dataset/training_set',  # Path to the training set directory\n",
    "    target_size = (64, 64),  # Resize images to 64x64 pixels\n",
    "    batch_size = 32,  # Number of images to process in a batch\n",
    "    class_mode = 'binary'  # Use categorical labels for multi-class classification\n",
    ")\n"
   ],
   "id": "8e141f51e7311a4c",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/training_set'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 11\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Apply transformations to the training set images to avoid overfitting\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Also named as data augmentation\u001B[39;00m\n\u001B[1;32m      4\u001B[0m train_datagen \u001B[38;5;241m=\u001B[39m ImageDataGenerator(\n\u001B[1;32m      5\u001B[0m     rescale \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m255\u001B[39m,  \u001B[38;5;66;03m# Normalize pixel values to [0, 1]; Feature scaling\u001B[39;00m\n\u001B[1;32m      6\u001B[0m     shear_range \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m,  \u001B[38;5;66;03m# Shear transformation; \u001B[39;00m\n\u001B[1;32m      7\u001B[0m     zoom_range \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m,  \u001B[38;5;66;03m# Zoom transformation\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     horizontal_flip \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# Random horizontal flip\u001B[39;00m\n\u001B[1;32m      9\u001B[0m )\n\u001B[0;32m---> 11\u001B[0m training_set \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_datagen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflow_from_directory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdataset/training_set\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Path to the training set directory\u001B[39;49;00m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Resize images to 64x64 pixels\u001B[39;49;00m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Number of images to process in a batch\u001B[39;49;00m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbinary\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Use categorical labels for multi-class classification\u001B[39;49;00m\n\u001B[1;32m     16\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/c o d e/DATA SCIENCE/ML A-Z/Machine-Learning-A-Z-Codes-Datasets/.venv/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:1138\u001B[0m, in \u001B[0;36mImageDataGenerator.flow_from_directory\u001B[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001B[0m\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mflow_from_directory\u001B[39m(\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1122\u001B[0m     directory,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1136\u001B[0m     keep_aspect_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1137\u001B[0m ):\n\u001B[0;32m-> 1138\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDirectoryIterator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1139\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1140\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1142\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolor_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolor_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1143\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_aspect_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_aspect_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1144\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1145\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1146\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1150\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_to_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_to_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1151\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1152\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1153\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_links\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_links\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1154\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1155\u001B[0m \u001B[43m        \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1157\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/c o d e/DATA SCIENCE/ML A-Z/Machine-Learning-A-Z-Codes-Datasets/.venv/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:453\u001B[0m, in \u001B[0;36mDirectoryIterator.__init__\u001B[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001B[0m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m classes:\n\u001B[1;32m    452\u001B[0m     classes \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 453\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m subdir \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m)\u001B[49m):\n\u001B[1;32m    454\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(directory, subdir)):\n\u001B[1;32m    455\u001B[0m             classes\u001B[38;5;241m.\u001B[39mappend(subdir)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'dataset/training_set'"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing the Test set",
   "id": "69d22077c5e82549"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:10:45.312852Z",
     "start_time": "2025-07-29T08:18:02.810187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)  # Normalize pixel values for the test set\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'dataset/test_set',  # Path to the test set directory\n",
    "    target_size = (64, 64),  # Resize images to 64x64\n",
    "    batch_size = 32,  # Number of images to process in a batch\n",
    "    class_mode = 'binary'  # Use categorical labels for multi-class classification\n",
    ")\n"
   ],
   "id": "2a1ab44f236d07c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 2: Building the CNN",
   "id": "60ddaf13ce19fb8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initializing the CNN",
   "id": "3033624992afb802"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:10:45.554794Z",
     "start_time": "2025-07-29T09:10:45.546697Z"
    }
   },
   "cell_type": "code",
   "source": "cnn = tf.keras.models.Sequential()  # Create a sequential model for the CNN\n",
   "id": "8a8d9746250b7c20",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 1: Convolution",
   "id": "2e55e62b754e3d3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:18:06.734577Z",
     "start_time": "2025-07-29T08:18:06.523412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters = 32,  # Number of filters in the convolutional layer because it is the first layer\n",
    "    kernel_size = (3, 3),  # Size of the convolutional kernel\n",
    "    activation = 'relu',  # Activation function for the layer we choose ReLU because it is a common choice for CNNs\n",
    "    input_shape = (64, 64, 3)  # Input shape of the images (64x64 pixels, 3 color channels) we choose 3 because the images are RGB images and 64x64 is the size of the images\n",
    "    \n",
    "))"
   ],
   "id": "73626150eef3678d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/octaviodaniel/Desktop/c o d e/DATA SCIENCE/ML A-Z/Machine-Learning-A-Z-Codes-Datasets/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 2: Pooling",
   "id": "a8e85f66870a1fd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:20:37.133012Z",
     "start_time": "2025-07-29T08:20:37.122150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size = 2, # Size of the pooling window, we choose 2x2 because it is a common choice for CNNs, we take 2x2 frames from the feature map and take the maximum value from each frame to reduce the size of the feature map\n",
    "    \n",
    "    strides = 2  # Stride size, we choose 2 because it is a common choice for CNNs, it means that we move the pooling window by 2 pixels in both directions\n",
    "    \n",
    "))"
   ],
   "id": "4d23a0a1200d2904",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding a convolutional layer ",
   "id": "d42f76a4ef8f5700"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:21:07.900722Z",
     "start_time": "2025-07-29T08:21:07.870139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters = 32,  # Number of filters in the convolutional layer\n",
    "    kernel_size = (3, 3),  # Size of the convolutional kernel\n",
    "    activation = 'relu'  # Activation function for the layer\n",
    "))\n",
    "# Adding a pooling layer\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size = 2,  # Size of the pooling window\n",
    "    strides = 2  # Stride size\n",
    "))"
   ],
   "id": "e04e80f6c6c2db70",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3: Flattening",
   "id": "48c27f5cf3badff6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:22:09.088540Z",
     "start_time": "2025-07-29T08:22:09.074957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.add(tf.keras.layers.Flatten())  # Flatten the feature maps to a 1D vector for the fully connected layer\n",
    "# Flattening is necessary to convert the 2D feature maps into a 1D vector"
   ],
   "id": "2246fd524cacf910",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 4: Full Connection",
   "id": "4be31172e9118c07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:27:22.951736Z",
     "start_time": "2025-07-29T08:27:22.920048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.add(tf.keras.layers.Dense(\n",
    "    units = 128,  # Number of neurons in the fully connected layer\n",
    "    activation = 'relu'  # Activation function for the layer\n",
    "))\n"
   ],
   "id": "3f95d695776b2133",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 5: Output Layer",
   "id": "6a3169f976c88a28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:27:23.815249Z",
     "start_time": "2025-07-29T08:27:23.791038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.add(tf.keras.layers.Dense(\n",
    "    units = 1,  # Number of output neurons (1 for binary classification)\n",
    "    activation = 'sigmoid'  # Sigmoid activation function for binary classification\n",
    "))"
   ],
   "id": "4d7f7228ce6e6b29",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 3: Training the CNN",
   "id": "f725923b84d2aa97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compiling the CNN and evaluating the model",
   "id": "a0397288738814bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:27:27.222115Z",
     "start_time": "2025-07-29T08:27:27.211222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.compile(\n",
    "    optimizer = 'adam',  # Optimizer for the model, we choose Adam because it is a common choice for CNNs\n",
    "    loss = 'binary_crossentropy',  # Loss function for binary classification\n",
    "    metrics = ['accuracy']  # Metrics to evaluate the model\n",
    ")"
   ],
   "id": "e426b47e570fce2e",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training the CNN on the training set and evaluating it on the test set",
   "id": "3602e490ee3f069"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:46:45.801532Z",
     "start_time": "2025-07-29T08:27:29.631486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.fit(\n",
    "    x = training_set,  # Training data\n",
    "    validation_data = test_set,  # Validation data\n",
    "    epochs = 25  # Number of epochs to train the model\n",
    ")"
   ],
   "id": "63e34ea9382d1657",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/octaviodaniel/Desktop/c o d e/DATA SCIENCE/ML A-Z/Machine-Learning-A-Z-Codes-Datasets/.venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 214ms/step - accuracy: 0.5328 - loss: 0.7036 - val_accuracy: 0.5820 - val_loss: 0.6659\n",
      "Epoch 2/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m54s\u001B[0m 215ms/step - accuracy: 0.6360 - loss: 0.6415 - val_accuracy: 0.7070 - val_loss: 0.5882\n",
      "Epoch 3/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m57s\u001B[0m 230ms/step - accuracy: 0.6900 - loss: 0.5866 - val_accuracy: 0.7115 - val_loss: 0.5643\n",
      "Epoch 4/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 181ms/step - accuracy: 0.7210 - loss: 0.5495 - val_accuracy: 0.7120 - val_loss: 0.5654\n",
      "Epoch 5/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 181ms/step - accuracy: 0.7345 - loss: 0.5395 - val_accuracy: 0.7650 - val_loss: 0.4897\n",
      "Epoch 6/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 183ms/step - accuracy: 0.7587 - loss: 0.4961 - val_accuracy: 0.7445 - val_loss: 0.5289\n",
      "Epoch 7/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 193ms/step - accuracy: 0.7540 - loss: 0.4824 - val_accuracy: 0.7670 - val_loss: 0.4901\n",
      "Epoch 8/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 196ms/step - accuracy: 0.7840 - loss: 0.4592 - val_accuracy: 0.7865 - val_loss: 0.4768\n",
      "Epoch 9/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 195ms/step - accuracy: 0.7791 - loss: 0.4582 - val_accuracy: 0.7790 - val_loss: 0.4783\n",
      "Epoch 10/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m55s\u001B[0m 222ms/step - accuracy: 0.7981 - loss: 0.4268 - val_accuracy: 0.7765 - val_loss: 0.4817\n",
      "Epoch 11/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 210ms/step - accuracy: 0.7933 - loss: 0.4294 - val_accuracy: 0.7480 - val_loss: 0.5762\n",
      "Epoch 12/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 207ms/step - accuracy: 0.7978 - loss: 0.4303 - val_accuracy: 0.7910 - val_loss: 0.4638\n",
      "Epoch 13/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 207ms/step - accuracy: 0.8167 - loss: 0.3932 - val_accuracy: 0.7455 - val_loss: 0.5219\n",
      "Epoch 14/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 206ms/step - accuracy: 0.8227 - loss: 0.3831 - val_accuracy: 0.7995 - val_loss: 0.4537\n",
      "Epoch 15/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 199ms/step - accuracy: 0.8390 - loss: 0.3703 - val_accuracy: 0.7975 - val_loss: 0.4692\n",
      "Epoch 16/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m43s\u001B[0m 170ms/step - accuracy: 0.8426 - loss: 0.3499 - val_accuracy: 0.7950 - val_loss: 0.4790\n",
      "Epoch 17/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 157ms/step - accuracy: 0.8494 - loss: 0.3384 - val_accuracy: 0.7935 - val_loss: 0.4729\n",
      "Epoch 18/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 152ms/step - accuracy: 0.8548 - loss: 0.3296 - val_accuracy: 0.7925 - val_loss: 0.4897\n",
      "Epoch 19/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 155ms/step - accuracy: 0.8562 - loss: 0.3089 - val_accuracy: 0.7995 - val_loss: 0.4864\n",
      "Epoch 20/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 157ms/step - accuracy: 0.8659 - loss: 0.3029 - val_accuracy: 0.7820 - val_loss: 0.5164\n",
      "Epoch 21/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 161ms/step - accuracy: 0.8807 - loss: 0.2789 - val_accuracy: 0.7955 - val_loss: 0.4774\n",
      "Epoch 22/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 149ms/step - accuracy: 0.8907 - loss: 0.2743 - val_accuracy: 0.7955 - val_loss: 0.5088\n",
      "Epoch 23/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 142ms/step - accuracy: 0.8892 - loss: 0.2591 - val_accuracy: 0.8000 - val_loss: 0.5026\n",
      "Epoch 24/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 142ms/step - accuracy: 0.8978 - loss: 0.2484 - val_accuracy: 0.8020 - val_loss: 0.5044\n",
      "Epoch 25/25\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 187ms/step - accuracy: 0.9030 - loss: 0.2299 - val_accuracy: 0.7760 - val_loss: 0.5700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x132366e00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Making a single prediction",
   "id": "f51b4c7ea7a4cf8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:05:14.439218Z",
     "start_time": "2025-07-29T09:05:14.320065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img('/Users/octaviodaniel/Desktop/c o d e/DATA SCIENCE/ML A-Z/Machine-Learning-A-Z-Codes-Datasets/Part 8 - Deep Learning/Section 40 - Convolutional Neural Networks (CNN)/Python/dataset/test_set/dogs/dog.4003.jpg', target_size=(64, 64))  # Load the image and resize it to 64x64 pixels\n",
    "\n",
    "test_image = image.img_to_array(test_image)  # Convert the image to a numpy array\n",
    "\n",
    "test_image = np.expand_dims(test_image, axis=0)  # Add a batch dimension\n",
    "\n",
    "result = cnn.predict(test_image)  # Make a prediction on the image\n",
    "\n",
    "training_set.class_indices  # Get the class indices from the training set\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'  # If the prediction is 1, it is a dog\n",
    "else:\n",
    "    prediction = 'cat'  # If the prediction is 0, it is a cat \n",
    "    \n",
    "print(prediction)  # Print the prediction"
   ],
   "id": "f29bcdec53822a92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 51ms/step\n",
      "dog\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
